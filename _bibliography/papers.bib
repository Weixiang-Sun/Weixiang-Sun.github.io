---
---

@article{yuan2024mora,
  title={Mora: Enabling generalist video generation via a multi-agent framework},
  author={Yuan*, Zhengqing and Liu*, Yixin and Sun*, Weixiang and Cao*, Yihan and Jia, Haolong and Chen, Ruoxi and Li, Zhaoxu and Lin, Bin and Yuan, Li and He, Lifang and others},
  journal={arXiv preprint arXiv:2403.13248},
  year={2024},
  abstract={Text-to-video generation has made significant strides, but replicating the capabilities of advanced systems like OpenAI's Sora remains challenging due to their closed-source nature. Existing open-source methods struggle to achieve comparable performance, often hindered by ineffective agent collaboration and inadequate training data quality. In this paper, we introduce Mora, a novel multi-agent framework that leverages existing open-source modules to replicate Sora's functionalities. We address these fundamental limitations by proposing three key techniques: (1) multi-agent fine-tuning with a self-modulation factor to enhance inter-agent coordination, (2) a data-free training strategy that uses large models to synthesize training data, and (3) a human-in-the-loop mechanism combined with multimodal large language models for data filtering to ensure high-quality training datasets. Our comprehensive experiments on six video generation tasks demonstrate that Mora achieves performance comparable to Sora on VBench, outperforming existing open-source methods across various tasks. Specifically, in the text-to-video generation task, Mora achieved a Video Quality score of 0.800, surpassing Sora's 0.797 and outperforming all other baseline models across six key metrics. Additionally, in the image-to-video generation task, Mora achieved a perfect Dynamic Degree score of 1.00, demonstrating exceptional capability in enhancing motion realism and achieving higher Imaging Quality than Sora. These results highlight the potential of collaborative multi-agent systems and human-in-the-loop mechanisms in advancing text-to-video generation. More visualization results of our work are available at https://mora-2025.github.io/.},
  preview={mora.jpg},
  selected={true},
  google_scholar_id={Y0pCki6q_DkC}
}


@article{yuan2025efficientllm,
  title={EfficientLLM: Efficiency in Large Language Models},
  author={Yuan*, Zhengqing and Sun*, Weixiang and Liu*, Yixin and Zhou*, Huichi and Zhou*, Rong and Li, Yiyang and Zhang, Zheyuan and Song, Wei and Huang, Yue and Jia, Haolong and others},
  journal={arXiv preprint arXiv:2505.13840},
  year={2025},
  preview={efficientllm.jpg},
  google_scholar_id={LkGwnXOMwfcC},
  selected={true}
}


@article{sun2024bora,
  title={Bora: Biomedical generalist video generation model},
  author={Sun*, Weixiang and You*, Xiaocao and Zheng*, Ruizhe and Yuan, Zhengqing and Li, Xiang and He, Lifang and Li, Quanzheng and Sun, Lichao},
  journal={arXiv preprint arXiv:2407.08944},
  year={2024},
  selected={true},
  google_scholar_id={W7OEmFMy1HYC},
}
