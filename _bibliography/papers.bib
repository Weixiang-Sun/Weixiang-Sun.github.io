---
---

@article{yuan2024mora,
  title={Mora: Enabling generalist video generation via a multi-agent framework},
  author={Yuan*, Zhengqing and Liu*, Yixin and Sun*, Weixiang and Cao*, Yihan and Jia, Haolong and Chen, Ruoxi and Li, Zhaoxu and Lin, Bin and Yuan, Li and He, Lifang and others},
  journal={arXiv preprint arXiv:2403.13248},
  year={2024},
  abstract={Text-to-video generation has made significant strides, but replicating the capabilities of advanced systems like OpenAI's Sora remains challenging due to their closed-source nature. Existing open-source methods struggle to achieve comparable performance, often hindered by ineffective agent collaboration and inadequate training data quality. In this paper, we introduce Mora, a novel multi-agent framework that leverages existing open-source modules to replicate Sora's functionalities. We address these fundamental limitations by proposing three key techniques: (1) multi-agent fine-tuning with a self-modulation factor to enhance inter-agent coordination, (2) a data-free training strategy that uses large models to synthesize training data, and (3) a human-in-the-loop mechanism combined with multimodal large language models for data filtering to ensure high-quality training datasets. Our comprehensive experiments on six video generation tasks demonstrate that Mora achieves performance comparable to Sora on VBench, outperforming existing open-source methods across various tasks. Specifically, in the text-to-video generation task, Mora achieved a Video Quality score of 0.800, surpassing Sora's 0.797 and outperforming all other baseline models across six key metrics. Additionally, in the image-to-video generation task, Mora achieved a perfect Dynamic Degree score of 1.00, demonstrating exceptional capability in enhancing motion realism and achieving higher Imaging Quality than Sora. These results highlight the potential of collaborative multi-agent systems and human-in-the-loop mechanisms in advancing text-to-video generation. More visualization results of our work are available at https://mora-2025.github.io/.},
  preview={mora.jpg},
  selected={true},
  google_scholar_id={Y0pCki6q_DkC},
  code={https://github.com/lichao-sun/Mora}
}


@article{yuan2025efficientllm,
  title={EfficientLLM: Efficiency in Large Language Models},
  author={Yuan*, Zhengqing and Sun*, Weixiang and Liu*, Yixin and Zhou*, Huichi and Zhou*, Rong and Li, Yiyang and Zhang, Zheyuan and Song, Wei and Huang, Yue and Jia, Haolong and others},
  journal={arXiv preprint arXiv:2505.13840},
  year={2025},
  preview={efficientllm.jpg},
  google_scholar_id={LkGwnXOMwfcC},
  selected={true},
  website={https://dlyuangod.github.io/EfficientLLM/},
  code={https://github.com/DLYuanGod/EfficientLLM/tree/main}
}


@article{sun2024bora,
  title={Bora: Biomedical generalist video generation model},
  author={Sun*, Weixiang and You*, Xiaocao and Zheng*, Ruizhe and Yuan, Zhengqing and Li, Xiang and He, Lifang and Li, Quanzheng and Sun, Lichao},
  journal={arXiv preprint arXiv:2407.08944},
  year={2024},
  selected={true},
  google_scholar_id={W7OEmFMy1HYC},
}


@article{zhou2024ttt,
  title={Ttt-unet: Enhancing u-net with test-time training layers for biomedical image segmentation},
  author={Zhou*, Rong and Yuan*, Zhengqing and Yan*, Zhiling and Sun*, Weixiang and Zhang, Kai and Li, Yiwei and Ye, Yanfang and Li, Xiang and He, Lifang and Sun, Lichao},
  journal={arXiv preprint arXiv:2409.11299},
  year={2024},
  google_scholar_id={Tyk-4Ss8FVUC}
}


@article{sun2024medical,
  title={Medical Unlearnable Examples: Securing Medical Data from Unauthorized Training via Sparsity-Aware Local Masking},
  author={Sun, Weixiang and Liu, Yixin and Yan, Zhiling and Xu, Kaidi and Sun, Lichao},
  journal={arXiv preprint arXiv:2403.10573},
  year={2024},
  google_scholar_id={zYLM7Y9cAGgC}
}


@inproceedings{sun2023research,
  title={Research and application of improved neural network optimization algorithm},
  author={Sun, Weixiang},
  booktitle={Third International Symposium on Computer Engineering and Intelligent Communications (ISCEIC 2022)},
  volume={12462},
  pages={602--606},
  year={2023},
  organization={SPIE},
  google_scholar_id={d1gkVwhDpl0C}
}


@article{yan2025samed,
  title={SAMed-2: Selective Memory Enhanced Medical Segment Anything Model},
  author={Yan, Zhiling and Song, Sifan and Song, Dingjie and Li, Yiwei and Zhou, Rong and Sun, Weixiang and Chen, Zhennong and Kim, Sekeun and Ren, Hui and Liu, Tianming and others},
  journal={arXiv preprint arXiv:2507.03698},
  year={2025}
}

@article{zhang2025llms4all,
  title={LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines},
  author={Ye*, Yanfang and Zhang*, Zheyuan and Ma*, Tianyi and Wang*, Zehong and Li*, Yiyang and Hou*, Shifu and Sun*, Weixiang and Shi*, Kaiwen and Ma, Yijun and Song, Wei and Abbasi, Ahmed and others},
  journal={arXiv preprint arXiv:2509.19580},
  year={2025},
  selected={true},
}

@article{miao2024advlogo,
  title={Advlogo: Adversarial patch attack against object detectors based on diffusion models},
  author={Miao, Boming and Li, Chunxiao and Zhu, Yao and Sun, Weixiang and Wang, Zizhe and Wang, Xiaoyi and Xie, Chuanlong},
  journal={arXiv preprint arXiv:2409.07002},
  year={2024}
}
